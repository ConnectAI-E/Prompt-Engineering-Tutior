1
00:00:05,000 --> 00:00:08,200
Expanding is the task of taking a shorter piece of text

2
00:00:08,200 --> 00:00:11,140
such as a set of instructions or a list of topics

3
00:00:11,140 --> 00:00:14,900
and having the large language model generate a longer piece of text

4
00:00:14,900 --> 00:00:18,260
such as an email or an essay about some topic

5
00:00:18,260 --> 00:00:20,100
There are some great uses of this

6
00:00:20,100 --> 00:00:23,960
such as if you use a large language model as a brainstorming partner

7
00:00:23,960 --> 00:00:26,300
But I just also want to acknowledge that there's

8
00:00:26,300 --> 00:00:28,500
some problematic use cases of this

9
00:00:28,500 --> 00:00:32,500
such as if someone were to use it to generate a large amount of spam

10
00:00:32,500 --> 00:00:36,100
So when you use these capabilities of a large language model

11
00:00:36,100 --> 00:00:41,340
please use it only in a responsible way and in a way that helps people

12
00:00:41,340 --> 00:00:45,280
In this video, we'll go through an example of how you can use

13
00:00:45,280 --> 00:00:49,900
a language model to generate a personalized email based on some information

14
00:00:49,900 --> 00:00:53,260
The email is kind of self-proclaimed to be from an AI bot

15
00:00:53,260 --> 00:00:56,140
which as Andrew mentioned, is very important

16
00:00:56,140 --> 00:00:58,500
We're also going to use another one of

17
00:00:58,500 --> 00:01:01,260
the model's input parameters called temperature

18
00:01:01,260 --> 00:01:04,780
and this allows you to vary the degree of

19
00:01:04,780 --> 00:01:08,760
exploration and variety in the model's responses

20
00:01:08,760 --> 00:01:10,980
So let's get into it

21
00:01:10,980 --> 00:01:12,740
So before we get started

22
00:01:12,740 --> 00:01:15,780
we're going to do the usual setup

23
00:01:15,780 --> 00:01:18,400
So set up the OpenAI Python package

24
00:01:18,400 --> 00:01:22,060
and then also define our helper function, get_completion

25
00:01:22,060 --> 00:01:26,460
Now we're going to write a custom email response to a customer review

26
00:01:26,460 --> 00:01:29,940
So given a customer review and the sentiment

27
00:01:29,940 --> 00:01:32,680
we're going to generate a custom response

28
00:01:32,680 --> 00:01:37,100
Now we're going to use the language model to generate

29
00:01:37,100 --> 00:01:41,900
a custom email to a customer based on a customer review

30
00:01:41,900 --> 00:01:44,020
and the sentiment of the review

31
00:01:44,020 --> 00:01:47,100
So we've already extracted the sentiment

32
00:01:47,100 --> 00:01:52,740
using the kind of prompts that we saw in the inferring video

33
00:01:52,740 --> 00:01:57,020
and then this is the customer review for a blender

34
00:01:57,100 --> 00:02:02,740
Now we're going to customize the reply based on the sentiment

35
00:02:02,740 --> 00:02:05,180
So here the instruction is

36
00:02:05,180 --> 00:02:08,260
you are a customer service AI assistant

37
00:02:08,260 --> 00:02:11,180
Your task is to send an email reply to a valued customer

38
00:02:11,180 --> 00:02:14,540
Given the customer email delimited by three backticks

39
00:02:14,540 --> 00:02:16,860
generate a reply to thank the customer for their review

40
00:02:16,860 --> 00:02:18,780
If the sentiment is positive or neutral

41
00:02:18,780 --> 00:02:20,460
thank them for their review

42
00:02:20,460 --> 00:02:21,980
If the sentiment is negative

43
00:02:21,980 --> 00:02:25,260
apologize and suggest that they can reach out to customer service

44
00:02:25,260 --> 00:02:27,820
Make sure to use specific details from the review

45
00:02:27,820 --> 00:02:29,820
write in a concise and professional tone

46
00:02:29,820 --> 00:02:32,460
and sign the email as AI customer agent

47
00:02:32,460 --> 00:02:34,820
When you're using a language model

48
00:02:34,820 --> 00:02:37,500
to generate text that you're going to show to a user

49
00:02:37,500 --> 00:02:40,860
it's very important to have this kind of transparency

50
00:02:40,860 --> 00:02:45,700
and let the user know that the text they're seeing was generated by AI

51
00:02:45,700 --> 00:02:49,780
Then we'll just input the customer review and the review sentiment

52
00:02:49,780 --> 00:02:53,220
Also note that this part isn't necessarily important

53
00:02:53,220 --> 00:02:57,180
because we could actually use this prompt to also extract the review sentiment

54
00:02:57,180 --> 00:02:59,300
and then in a follow-up step, write the email

55
00:02:59,300 --> 00:03:01,140
But just for the sake of the example

56
00:03:01,140 --> 00:03:04,300
well, we've already extracted the sentiment from the review

57
00:03:04,300 --> 00:03:08,500
So here we have a response to the customer

58
00:03:08,500 --> 00:03:13,580
It addresses details that the customer mentioned in their review

59
00:03:13,580 --> 00:03:16,620
and as we instructed

60
00:03:16,620 --> 00:03:19,100
suggest that they reach out to customer service

61
00:03:19,100 --> 00:03:23,100
because this is just an AI customer service agent

62
00:03:23,740 --> 00:03:30,420
Next, we're going to use a parameter of the language model called temperature

63
00:03:30,420 --> 00:03:36,460
that will allow us to change the variety of the model's responses

64
00:03:36,460 --> 00:03:38,900
So you can think of temperature as

65
00:03:38,900 --> 00:03:43,380
the degree of exploration or randomness of the model

66
00:03:43,380 --> 00:03:46,060
So for this particular phrase

67
00:03:46,060 --> 00:03:47,500
my favorite food is

68
00:03:47,500 --> 00:03:51,820
the most likely next word that the model predicts is pizza

69
00:03:51,820 --> 00:03:53,580
and the next two most likely

70
00:03:53,580 --> 00:03:55,900
it suggests are sushi and tacos

71
00:03:55,900 --> 00:03:57,660
So at a temperature of zero

72
00:03:57,660 --> 00:04:00,060
the model will always choose the most likely next word

73
00:04:00,060 --> 00:04:01,460
which in this case is pizza

74
00:04:01,460 --> 00:04:03,260
and at a higher temperature

75
00:04:03,260 --> 00:04:07,100
it will also choose one of the less likely words

76
00:04:07,100 --> 00:04:09,340
and at an even higher temperature

77
00:04:09,340 --> 00:04:10,700
it might even choose tacos

78
00:04:10,700 --> 00:04:15,380
which only has a five percent chance of being chosen

79
00:04:15,380 --> 00:04:21,660
You can imagine that as the model continues this final response

80
00:04:21,660 --> 00:04:23,300
so my favorite food is pizza

81
00:04:23,300 --> 00:04:25,420
and it continues to generate more words

82
00:04:25,420 --> 00:04:30,580
this response will diverge from the first response

83
00:04:30,580 --> 00:04:32,340
which is my favorite food is tacos

84
00:04:32,340 --> 00:04:34,100
So as the model continues

85
00:04:34,100 --> 00:04:37,060
these two responses will become more and more different

86
00:04:37,060 --> 00:04:40,500
In general, when building applications where you want

87
00:04:40,500 --> 00:04:43,420
a predictable response

88
00:04:43,420 --> 00:04:45,420
I would recommend using temperature zero

89
00:04:45,420 --> 00:04:46,780
Throughout all of these videos

90
00:04:46,780 --> 00:04:48,900
we've been using temperature zero

91
00:04:48,900 --> 00:04:51,380
and I think that if you're trying to build a system that

92
00:04:51,380 --> 00:04:54,660
is reliable and predictable, you should go with this

93
00:04:54,660 --> 00:04:58,780
If you're trying to use the model in a more creative way

94
00:04:58,780 --> 00:05:04,220
where you might want a wider variety of different outputs

95
00:05:04,220 --> 00:05:06,860
you might want to use a higher temperature

96
00:05:06,860 --> 00:05:11,620
So now let's take this same prompt that we just used

97
00:05:11,620 --> 00:05:14,220
and let's try generating an e-mail

98
00:05:14,220 --> 00:05:16,140
but let's use a higher temperature

99
00:05:16,140 --> 00:05:21,540
So in our get_completion function that we've been using throughout the videos

100
00:05:21,540 --> 00:05:25,100
we have specified a model and then also a temperature

101
00:05:25,100 --> 00:05:26,540
but we've set them to default

102
00:05:26,540 --> 00:05:29,420
So now let's try varying the temperature

103
00:05:30,140 --> 00:05:32,860
So we use the prompt

104
00:05:32,860 --> 00:05:39,340
and then let's try temperature=0.7

105
00:05:43,620 --> 00:05:45,340
And so with temperature=0

106
00:05:45,340 --> 00:05:47,220
every time you execute the same prompt

107
00:05:47,220 --> 00:05:50,580
you should expect the same completion

108
00:05:50,580 --> 00:05:52,580
whereas with temperature 0.7

109
00:05:52,580 --> 00:05:55,060
you'll get a different output every time

110
00:05:55,060 --> 00:05:58,100
So here we have our e-mail

111
00:05:58,100 --> 00:06:00,980
and as you can see, it's different to the e-mail

112
00:06:00,980 --> 00:06:02,900
that we received previously

113
00:06:02,900 --> 00:06:08,060
And let's just execute it again to show that we'll get a different e-mail again

114
00:06:08,060 --> 00:06:11,340
And here we have another different e-mail

115
00:06:11,340 --> 00:06:16,780
So I recommend that you play around with temperature yourself

116
00:06:16,780 --> 00:06:20,820
Maybe you could pause the video now and try this prompt

117
00:06:20,820 --> 00:06:22,620
with a variety of different temperatures

118
00:06:22,620 --> 00:06:25,100
just to see how the outputs vary

119
00:06:25,100 --> 00:06:28,500
So to summarize, at higher temperatures

120
00:06:28,500 --> 00:06:31,700
the outputs from the model are kind of more random

121
00:06:31,700 --> 00:06:34,500
You can almost think of it as that at higher temperatures

122
00:06:34,500 --> 00:06:36,500
the assistant is more distractible

123
00:06:36,500 --> 00:06:38,220
but maybe more creative

124
00:06:39,620 --> 00:06:42,260
In the next video, we're going to talk more about

125
00:06:42,260 --> 00:06:44,540
the chat completions endpoint format

126
00:06:44,540 --> 00:07:00,060
and how you can create a custom chatbot using this format

